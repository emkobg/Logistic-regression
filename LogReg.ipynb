{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoNorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Bobev/Desktop/Python/Logistic regression/LogReg.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Bobev/Desktop/Python/Logistic%20regression/LogReg.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m X_norm\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Bobev/Desktop/Python/Logistic%20regression/LogReg.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Normalize the training and testing sets using min-max normalization\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Bobev/Desktop/Python/Logistic%20regression/LogReg.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m X_train_norm, X_train_ranges, X_train_minvals \u001b[39m=\u001b[39m autoNorm(X_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Bobev/Desktop/Python/Logistic%20regression/LogReg.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m X_test_norm \u001b[39m=\u001b[39m (X_test \u001b[39m-\u001b[39m X_train_minvals) \u001b[39m/\u001b[39m X_train_ranges\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Bobev/Desktop/Python/Logistic%20regression/LogReg.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Train the logistic regression model using gradient ascent\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoNorm' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the Palmer Penguins dataset\n",
    "penguins = pd.read_csv('penguins.csv')\n",
    "\n",
    "# Recode the 'sex' column to binary labels 0 and 1\n",
    "penguins['Sex'] = penguins['Sex'].replace(to_replace=['MALE', 'FEMALE'], value=[0, 1])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = penguins.iloc[:, [0, 1, 2]].to_numpy()  # select the first 3 columns as features\n",
    "y = penguins.iloc[:, 3].to_numpy()         # select the 'sex' column as the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the gradient ascent algorithm\n",
    "def grad_ascent(data, labels, alpha=0.001, max_iter=500):\n",
    "    data_matrix = np.mat(data)\n",
    "    label_matrix = np.mat(labels).transpose()\n",
    "    m, n = np.shape(data_matrix)\n",
    "    weights = np.ones((n, 1))\n",
    "    for i in range(max_iter):\n",
    "        h = sigmoid(data_matrix * weights)\n",
    "        error = label_matrix - h\n",
    "        weights = weights + alpha * data_matrix.transpose() * error\n",
    "    return weights\n",
    "\n",
    "# Define the predict function\n",
    "def predict(X, weights):\n",
    "    return np.where(sigmoid(X @ weights) > 0.5, 1, 0)\n",
    "\n",
    "# Normalize the training set using min-max normalization\n",
    "def normalize(X):\n",
    "    X_norm = X.copy()\n",
    "    num_features = X.shape[1]\n",
    "    for i in range(num_features):\n",
    "        min_val = np.min(X[:, i])\n",
    "        max_val = np.max(X[:, i])\n",
    "        X_norm[:, i] = (X[:, i] - min_val) / (max_val - min_val)\n",
    "    return X_norm\n",
    "\n",
    "# Normalize the training and testing sets using min-max normalization\n",
    "X_train_norm, X_train_ranges, X_train_minvals = autoNorm(X_train)\n",
    "X_test_norm = (X_test - X_train_minvals) / X_train_ranges\n",
    "\n",
    "# Train the logistic regression model using gradient ascent\n",
    "X_train_norm = np.hstack((np.ones((X_train_norm.shape[0], 1)), X_train_norm))  # add a column of 1s for the bias term\n",
    "weights = grad_ascent(X_train_norm, y_train)\n",
    "\n",
    "# Evaluate the model's performance on the testing set\n",
    "X_test_norm = np.hstack((np.ones((X_test_norm.shape[0], 1)), X_test_norm))  # add a column of 1s for the bias term\n",
    "y_pred = predict(X_test_norm, weights)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.scatter(X_train_norm[:, 1], X_train_norm[:, 2], c=y_train)\n",
    "x1 = np.min(X_train_norm[:, 1]) - 0.1\n",
    "x2 = np.max(X_train_norm[:, 1]) + 0.1\n",
    "y1 = (-weights[0] - weights[1] * x1) / weights[2]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
